{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPALlIrOLC3TYYNUR3HPAMM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uzairafnan007/2/blob/main/BDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA_07olgfzbX",
        "outputId": "1301f3e3-5bbc-4b81-da04-5da94651d72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the window size (N): 24\n",
            "Enter the binary string: 0101100010111011100101101\n",
            "Estimated number of 1's in the last 24 bits: 13\n",
            "Total number of buckets: 3\n",
            "Total count of 1's in all buckets: 14\n",
            "Buckets (size, bits):\n",
            "Size: 8, Bits: 11111111\n",
            "Size: 4, Bits: 1111\n",
            "Size: 2, Bits: 11\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "class DGIM:\n",
        "    def __init__(self, window_size):\n",
        "        self.window_size = window_size\n",
        "        self.buckets = []\n",
        "\n",
        "    def add_bit(self, bit):\n",
        "        # Slide the window\n",
        "        if len(self.buckets) > 0 and self.buckets[0][1] >= self.window_size:\n",
        "            self.buckets.pop(0)\n",
        "\n",
        "        # Add the new bit if it's 1\n",
        "        if bit == 1:\n",
        "            self.buckets.append((1, 0))  # (size, age)\n",
        "            self.merge_buckets()\n",
        "\n",
        "        # Update the age of all buckets\n",
        "        for i in range(len(self.buckets)):\n",
        "            self.buckets[i] = (self.buckets[i][0], self.buckets[i][1] + 1)\n",
        "\n",
        "    def merge_buckets(self):\n",
        "        # Merge buckets of the same size\n",
        "        while len(self.buckets) >= 2 and self.buckets[-1][0] == self.buckets[-2][0]:\n",
        "            size, age = self.buckets.pop()\n",
        "            self.buckets[-1] = (size * 2, age)\n",
        "\n",
        "    def count_ones(self):\n",
        "        total = 0\n",
        "        last_bucket_size = self.buckets[-1][0] // 2 if self.buckets else 0\n",
        "\n",
        "        for size, age in self.buckets[:-1]:\n",
        "            total += size\n",
        "        return total + last_bucket_size\n",
        "\n",
        "    def print_buckets(self):\n",
        "        print(\"Buckets (size, bits):\")\n",
        "        for size, age in self.buckets:\n",
        "            print(f\"Size: {size}, Bits: {'1' * size}\")\n",
        "\n",
        "# Main function to simulate the input/output like the example\n",
        "def main():\n",
        "    N = int(input(\"Enter the window size (N): \"))\n",
        "    binary_string = input(\"Enter the binary string: \")\n",
        "\n",
        "    dgim = DGIM(window_size=N)\n",
        "\n",
        "    for bit in binary_string:\n",
        "        dgim.add_bit(int(bit))\n",
        "\n",
        "    estimated_ones = dgim.count_ones()\n",
        "\n",
        "    print(f\"Estimated number of 1's in the last {N} bits: {estimated_ones}\")\n",
        "    print(f\"Total number of buckets: {len(dgim.buckets)}\")\n",
        "    print(f\"Total count of 1's in all buckets: {sum(size for size, _ in dgim.buckets)}\")\n",
        "    dgim.print_buckets()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def trailing_zeros(x):\n",
        "    return len(bin(x)[::-1].split('1', 1)[0])\n",
        "\n",
        "def flajolet_martin(dataset, k):\n",
        "    sampled_elements = [random.choice(dataset) for _ in range(len(dataset) * k)]\n",
        "    max_zeros = max(trailing_zeros(x) for x in sampled_elements)\n",
        "    return sampled_elements, 2 ** max_zeros\n",
        "\n",
        "# Example usage\n",
        "dataset = [1, 1, 1, 1, 1, 1, 1, 4, 3, 2]  # Replace with your dataset\n",
        "sampled_elements, dist_num = flajolet_martin(dataset, 10)\n",
        "print(\"Sampled elements:\", sampled_elements)\n",
        "print(\"Estimated number of distinct elements:\", dist_num)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KItQhlm1jKzR",
        "outputId": "33adddb1-d38e-4de1-f9e6-a099b3897d28"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled elements: [1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 4, 1, 1, 1, 4, 1, 1, 3, 1, 1, 4, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 4, 4, 1, 1, 2, 1, 1, 1, 3, 3, 2, 1, 1, 1, 3, 4, 4, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 3, 1, 1, 1]\n",
            "Estimated number of distinct elements: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import sys\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "\n",
        "class WordMapper:\n",
        "    def __init__(self):\n",
        "        self.word_counts = []\n",
        "\n",
        "    def clean_word(self, word: str) -> str:\n",
        "        \"\"\"Clean a word by converting to lowercase and removing special characters.\"\"\"\n",
        "        word = word.lower()\n",
        "        word = re.sub(r'[^a-z0-9]', '', word)\n",
        "        return word\n",
        "\n",
        "    def process_line(self, line: str) -> List[Tuple[str, int]]:\n",
        "        \"\"\"Process a single line of text and return word-count pairs.\"\"\"\n",
        "        words = line.strip().split()\n",
        "        return [(self.clean_word(word), 1) for word in words if self.clean_word(word)]\n",
        "\n",
        "    def map_input(self) -> None:\n",
        "        \"\"\"Read from stdin and emit word-count pairs.\"\"\"\n",
        "        for line in sys.stdin:\n",
        "            word_counts = self.process_line(line)\n",
        "            for word, count in word_counts:\n",
        "                print(f\"{word}\\t{count}\")\n",
        "\n",
        "def main():\n",
        "    mapper = WordMapper()\n",
        "    mapper.map_input()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "A5YTO-CWvt2Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import sys\n",
        "from typing import Dict, Generator\n",
        "from collections import defaultdict\n",
        "\n",
        "class WordReducer:\n",
        "    def __init__(self):\n",
        "        self.word_counts = defaultdict(int)\n",
        "\n",
        "    def parse_line(self, line: str) -> tuple[str, int]:\n",
        "        \"\"\"Parse a line of input into word and count.\"\"\"\n",
        "        try:\n",
        "            word, count = line.strip().split('\\t')\n",
        "            return word, int(count)\n",
        "        except ValueError:\n",
        "            print(f\"Warning: Skipping malformed line: {line}\", file=sys.stderr)\n",
        "            return None, None\n",
        "\n",
        "    def read_input(self) -> Generator[tuple[str, int], None, None]:\n",
        "        \"\"\"Read and parse input lines from stdin.\"\"\"\n",
        "        for line in sys.stdin:\n",
        "            word, count = self.parse_line(line)\n",
        "            if word is not None:\n",
        "                yield word, count\n",
        "\n",
        "    def reduce(self) -> None:\n",
        "        \"\"\"Reduce word-count pairs and print results.\"\"\"\n",
        "        # Aggregate counts\n",
        "        for word, count in self.read_input():\n",
        "            self.word_counts[word] += count\n",
        "\n",
        "        # Print results in sorted order\n",
        "        for word in sorted(self.word_counts.keys()):\n",
        "            print(f\"{word}\\t{self.word_counts[word]}\")\n",
        "\n",
        "    def get_total_words(self) -> int:\n",
        "        \"\"\"Return total number of words processed.\"\"\"\n",
        "        return sum(self.word_counts.values())\n",
        "\n",
        "    def get_unique_words(self) -> int:\n",
        "        \"\"\"Return number of unique words.\"\"\"\n",
        "        return len(self.word_counts)\n",
        "\n",
        "def main():\n",
        "    reducer = WordReducer()\n",
        "    reducer.reduce()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "tYF5zMwFvx90"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've created two separate, full-featured programs for word counting. Here's how to use them:\n",
        "\n",
        "Save them as word_mapper.py and word_reducer.py\n",
        "Make them executable:\n",
        "\n",
        "bashCopychmod +x word_mapper.py word_reducer.py\n",
        "\n",
        "Use them in a pipeline:\n",
        "\n",
        "bashCopycat input.txt | ./word_mapper.py | sort | ./word_reducer.py"
      ],
      "metadata": {
        "id": "4Wef2SAKv2qR"
      }
    }
  ]
}